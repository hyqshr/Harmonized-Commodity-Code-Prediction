{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477e7d38",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "descs = pd.read_csv(\"descs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c795501",
   "metadata": {},
   "outputs": [],
   "source": [
    "hscodes = pd.read_excel(\"hscodes.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea56662",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb300 = pd.concat([hscodes,descs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50438029",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb300['Description'] = df_xgb300['Description'].astype(str).str.lower()\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('hs code')[0])\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('hts code')[0])\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('hscode')[0])\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('h s code')[0])\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('hs-code')[0])\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('hs.code')[0])\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('h.s.code')[0])\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('htscode')[0])\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('hts-code')[0])\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('hts.code')[0])\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: re.sub(r'\\d{4}\\.\\d{2}','', x))\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: re.sub(r'\\d{6}|\\d{10}','', x))\n",
    "df_xgb300['Description'] = df_xgb300['Description'].str.replace('\\W', ' ')\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('hts')[0])\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('hs')[0])\n",
    "df_xgb300['Description'] = df_xgb300['Description'].apply(lambda x: x.split('h s')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc935cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the set of stop words from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stop words\n",
    "stopwords = {word.lower() for word in stopwords.words('english')}\n",
    "\n",
    "# Assuming you have a DataFrame 'df_xgb300' with a column 'text_column'\n",
    "# ...\n",
    "\n",
    "def remove_punctuation_and_stopwords(text):\n",
    "    # Remove punctuation\n",
    "    no_punct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # Remove stop words\n",
    "    no_stopwords = \" \".join([word for word in no_punct.split() if word not in stopwords])\n",
    "\n",
    "    return no_stopwords\n",
    "\n",
    "# Apply the function to your text column\n",
    "df_xgb300['Description'] = df_xgb300['Description'].astype(str).apply(remove_punctuation_and_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the target column\n",
    "grouped = df_xgb300.groupby('HS_Code')\n",
    "\n",
    "# Filter groups with a size of at least 5\n",
    "filtered_groups = grouped.filter(lambda x: len(x) >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ee53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb300['HS_Code'] = df_sample4['HS_Code'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dfacd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('roberta-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# Initialize the KeyBERT model with the RoBERTa-based model\n",
    "keybert_model = KeyBERT(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9abc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text, num_keywords=10):\n",
    "    return ' '.join([kw[0] for kw in keybert_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), top_n=num_keywords)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7056c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_list =[]\n",
    "for desc in df_xgb300['Description']:\n",
    "    kw_list.append(extract_keywords(desc),50)\n",
    "\n",
    "df_xgb300['Keywords'] = kw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_100 = SMOTE(k_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_list4 =[]\n",
    "for desc in df_xgb300['Description']:\n",
    "    kw_list4.append(extract_keywords(desc,50))\n",
    "\n",
    "df_xgb300['Keywords'] = kw_list4\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_xgb300['Labels_Enc'] = label_encoder.fit_transform(df_xgb300['HS_Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67340a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', \n",
    "                                   num_class=len(np.unique(df_xgb300['HS_Code'])), \n",
    "                                   n_estimators  = 250, \n",
    "                                   max_depth = 20, \n",
    "                                   learning_rate = 0.2)\n",
    "# Define Stratified K-Fold cross-validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_accuracies = []\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "all_y_test = []\n",
    "all_y_pred_probs = []\n",
    "\n",
    "i=1\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in stratified_kfold.split(df_xgb300['Keywords'], df_xgb300['Labels_Enc']):\n",
    "    # Split data\n",
    "    X_train, X_initial_test = df_xgb300.iloc[train_index]['Keywords'], df_xgb300.iloc[test_index]['Keywords']\n",
    "    y_train, y_initial_test = df_xgb300.iloc[train_index]['Labels_Enc'], df_xgb300.iloc[test_index]['Labels_Enc']\n",
    "\n",
    "    # Further split the initial test set into test and validation sets\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_initial_test, y_initial_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    X_val_tfidf = vectorizer.transform(X_val)\n",
    "\n",
    "    start_samp = time.time()\n",
    "    X_train_smote, y_train_smote = smote_100.fit_resample(X_train_tfidf, y_train)\n",
    "    end_samp = time.time()\n",
    "\n",
    "    print(\" Sampling time till 30: \", end_samp - start_samp)\n",
    "\n",
    "    start_cv = time.time()\n",
    "    # Train model\n",
    "    xgb_classifier.fit(X_train_smote, y_train_smote, early_stopping_rounds=10, eval_set=[(X_val_tfidf, y_val)], eval_metric='mlogloss', verbose=True)\n",
    "    end_cv = time.time()\n",
    "\n",
    "    print(\"Fold \", i, \"fitting time: \", end_cv - start_cv)\n",
    "\n",
    "    i=i+1\n",
    "    # Predict on test set\n",
    "    y_pred = xgb_classifier.predict(X_test_tfidf)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    y_train_pred = xgb_classifier.predict(X_train_tfidf)\n",
    "    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n",
    "    y_pred_probs = xgb_classifier.predict_proba(X_test_tfidf)\n",
    "    all_y_pred_probs.append(y_pred_probs)\n",
    "    all_y_test.extend(y_test)\n",
    "\n",
    "print(\"Cross-validation time: \", end_cv - start_cv)\n",
    "\n",
    "# Displaying average results\n",
    "print(f\"Average Accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores)}\")\n",
    "\n",
    "print(\"Train accuracies: \",train_accuracies)\n",
    "print(\"Test accuracies: \", accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47213fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_accuracy(y_pred_probs, y_true, n):\n",
    "    top_n_preds = np.argsort(y_pred_probs, axis=1)[:, -n:]\n",
    "    return np.mean([y_true[i] in top_n_preds[i] for i in range(len(y_true))])\n",
    "\n",
    "# Calculate and print top-N accuracies\n",
    "for n in range(1, 10):\n",
    "    accuracy = top_n_accuracy(y_pred_probs, y_test, n)\n",
    "    print(f\"Top-{n} Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069b05f",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('lr', LogisticRegression()))\n",
    "    level0.append(('svm', SVC()))\n",
    "    level0.append(('dt', DecisionTreeClassifier(random_state=42)))\n",
    "\t  # define meta learner model\n",
    "    level1 = LogisticRegression()\n",
    "\t  # define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['lr'] = LogisticRegression()\n",
    "    models['svm'] = SVC(class_weight = 'balanced', gamma = 0.1, kernel = 'rbf')\n",
    "    models['dt'] = DecisionTreeClassifier(random_state=42)\n",
    "    models['stacking'] = get_stacking()\n",
    "    return models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        # Split data\n",
    "        X = X.astype(str)\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Apply TF-IDF\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        print(\"TFIDF train fit start\")\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        print(\"TFIDF train fit end\")\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "        print(\"TFIDF test transform end\")\n",
    "\n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE(k_neighbors=3)\n",
    "        X_train_tfidf_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "        # Fit and predict\n",
    "        model_clone = clone(model)  # Clone the model to ensure independence between folds\n",
    "        model_clone.fit(X_train_tfidf_smote, y_train_smote)\n",
    "        y_pred = model_clone.predict(X_test_tfidf)\n",
    "\n",
    "        # Evaluate\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        break\n",
    "\n",
    "    return np.array(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ed05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "X, y = df_xgb300['Keywords'], df_xgb300['Labels_Enc']\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e554dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print(name, mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HS-Code Project Env",
   "language": "python",
   "name": "env_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
